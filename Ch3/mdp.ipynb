{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb45705-b94a-46d5-8b02-418887b40e7e",
   "metadata": {},
   "source": [
    "# Markov Decission Problem (MDP) policy evaluaiton methods (state value V and state-action Q values, policy improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c251b-0724-44d9-9f9d-0e5ac0fd0120",
   "metadata": {},
   "source": [
    "### Legacy repo does not work with gymnasium so some installs and hacks are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279e608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b73a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym_walk\n",
    "#!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7f658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import gymnasium as gym, gym_walk\n",
    "#import gymnasium as gym\n",
    "#from gym_walk.envs import WalkEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0a3439-383d-4da9-9273-c4a7dafbd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gymnasium.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ec3d92-b836-4573-869a-87950d29dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#register(\n",
    "#    # One non-terminal states and two terminal\n",
    "#    # Technically speaking, this is a bandit MDP\n",
    "#    id='BanditWalk-v0',\n",
    "#    entry_point='gym_walk.envs:WalkEnv',\n",
    "#    # left-most and right-most states are terminal\n",
    "#    kwargs={'n_states': 1, 'p_stay': 0.0, 'p_backward': 0.0},\n",
    "#    max_episode_steps=100,\n",
    "#    reward_threshold=1.0,\n",
    "#    nondeterministic=True,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef7fb69a-a565-4abf-a1d2-58ec39f33916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#register(\n",
    "#    # One non-terminal states and two terminal\n",
    "#    # Technically speaking, this is a bandit MDP\n",
    "#    id='BanditSlipperyWalk-v0',\n",
    "#    entry_point='gym_walk.envs:WalkEnv',\n",
    "#    # left-most and right-most states are terminal\n",
    "#    kwargs={'n_states': 1, 'p_stay': 0.0, 'p_backward': 0.2},\n",
    "#    max_episode_steps=100,\n",
    "#    reward_threshold=1.0,\n",
    "#    nondeterministic=True,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c76394b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Stochastic) Markov Decission Problem: MDP\n",
    "## s a (p(a|s), s', r, Teminal?)\n",
    "#P = {\n",
    "#    0: {\n",
    "#        0: [(1.0, 0, 0.0, True)],\n",
    "#        1: [(1.0, 0, 0.0, True)]\n",
    "#    },\n",
    "#    1: {\n",
    "#        0: [(0.8, 0, 0.0, True), (0.2, 2, 1.0, True)],\n",
    "#        1: [(0.8, 2, 1.0, True), (0.2, 0, 0.0, True)]\n",
    "#    },\n",
    "#    2: {\n",
    "#        0: [(1.0, 2, 0.0, True)],\n",
    "#        1: [(1.0, 2, 0.0, True)]\n",
    "#    }\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47da04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('BanditWalk-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea547b3f-ab4d-4a14-a86c-e2336986e9bf",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5687dc-4a5b-4fe7-afaf-c477a3597454",
   "metadata": {},
   "source": [
    "### Upon installing gym_walk lib one can actuall directly instantiated envs used in the book by using proper env parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e6b665-3c3f-407a-9f34-33dcafb984ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym_walk.envs import WalkEnv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04ba75-01d5-415d-afc1-31ad3a0dc184",
   "metadata": {},
   "source": [
    "### Slippery walk 1D AKA two-arm stochastic bandid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf79675-13c1-49dd-aa8a-64c67cd315bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WalkEnv(n_states=1, p_stay=0.0, p_backward=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a00471-f724-4438-adfc-595b82e5c904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.8, 0, 0.0, True), (0.0, 0, 0.0, True), (0.2, 1, 0.0, False)],\n",
       "  1: [(0.8, 1, 0.0, False), (0.0, 0, 0.0, True), (0.2, 0, 0.0, True)]},\n",
       " 1: {0: [(0.8, 0, 0.0, True), (0.0, 1, 0.0, False), (0.2, 2, 1.0, True)],\n",
       "  1: [(0.8, 2, 1.0, True), (0.0, 1, 0.0, False), (0.2, 0, 0.0, True)]},\n",
       " 2: {0: [(0.8, 1, 0.0, False), (0.0, 2, 0.0, True), (0.2, 2, 0.0, True)],\n",
       "  1: [(0.8, 2, 0.0, True), (0.0, 2, 0.0, True), (0.2, 1, 0.0, False)]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7871a-349b-487a-a796-85e369ab01d8",
   "metadata": {},
   "source": [
    "## Policy evaluation - iterative value function estimation from MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3249e-4cc2-48c3-9510-db055d70ab92",
   "metadata": {},
   "source": [
    "### Trivial random policy for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "24c8942b-5687-442f-b3a7-e3b529c88f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = lambda x: np.random.randint(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b6f938d7-bb44-44de-a3b9-dd53a3b44a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p - policy, a probability distribution over actions give a state\n",
    "# P - MDP\n",
    "def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n",
    "    \n",
    "    # len(P) - dimension of states space\n",
    "    states_dim = len(P)\n",
    "    prev_V = np.zeros(states_dim)\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        i+=1\n",
    "        V = np.zeros(states_dim)\n",
    "        for s in range(states_dim):\n",
    "            # Iterate over each fan-out of actions out of the given state in MDP P under policy p\n",
    "            for prob, next_state, reward, done in P[s][pi(s)]:\n",
    "                #print(reward)\n",
    "                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n",
    "                #print(V)\n",
    "        #if np.max(np.abs(V - prev_V)) < theta:\n",
    "        #    break\n",
    "        if i >1000:\n",
    "            break\n",
    "        #print(V)\n",
    "        prev_V = V.copy()\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "785081d0-adf5-47a1-8988-6c924c75f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_V = [policy_evaluation(pi, env.P) for _ in range(1)]\n",
    "V_star = np.mean(ret_V, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374226ab-a8eb-4c28-9621-efd07230c157",
   "metadata": {},
   "source": [
    "## Policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "71dea3e3-fa7a-4794-80b9-0f723145a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvment(V, P, gamma=1.0):\n",
    "    Q = np.zeros((len(P), len(P[0])))\n",
    "    for s in range(len(P)):\n",
    "        for a in range(len(P[s])):\n",
    "            for prob, next_state, reward, done in P[s][a]:\n",
    "                Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2dd28a9c-5efd-4ac2-9adf-076d24e3c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_star = policy_improvment(V_star, env.P, gamma=1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "276a41ec-3b75-4d8f-a3b6-43fe5f62b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_tab = [V_star]\n",
    "p_tab = [pi]\n",
    "\n",
    "for _ in range(30):\n",
    "    V_tab += [policy_evaluation(p_tab[-1], env.P)]\n",
    "    p_tab += [policy_improvment(V_tab[-1], env.P)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c503be88-71ce-48e3-8571-cb886e6f1122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64, 0.8 , 0.64])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_tab[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "85446f49-6ce6-452d-8ecb-9d41c85af6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p_tab[-1](0), p_tab[-1](1), p_tab[-1](2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bba240-2be1-4498-b912-9a1b5d328300",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ac55b6f0-084f-441e-ad55-bca89d39d566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action space\n",
    "tuple(env.P[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "97b0af3d-59ac-4c77-99c2-bafa42679f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random policy assigning action (from env.P[0].keys())) to each of states (dim = len(env.P)) \n",
    "np.random.choice(tuple(env.P[0].keys()), len(env.P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6e48e7e3-c2d3-4853-a343-8ea54ea5d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, gamma=1.0, theta=1e-10):\n",
    "    # Randomly choose action from the allowed set (here (0, 1)) and assign to every state (here (0, 1, 2))\n",
    "    random_actions = np.random.choice(tuple(P[0].keys()), len(P))\n",
    "    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]\n",
    "    while True:\n",
    "        # Store previous policy in a dict\n",
    "        old_pi = {s:pi(s) for s in range(len(P))}\n",
    "        V = policy_evaluation(pi, P, gamma, theta)\n",
    "        pi = policy_improvment(V, P, gamma)\n",
    "        # Break loop if no change can be made - risky for large stochastic envs?\n",
    "        if old_pi == {s:pi(s) for s in range(len(P))}:\n",
    "            break\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a847808a-a843-41f4-9d94-1f5afc1fefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_star, pi_star = policy_iteration(env.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "35027d67-5ee0-4513-9b60-4814c5bf303d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64, 0.8 , 0.64])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11872686-0c76-4567-a479-3d590397fd9a",
   "metadata": {},
   "source": [
    "### This agrees with the above naive method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5642879d-5075-456d-8edb-0abfc7ab16bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 1, 2: 0}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{s:pi_star(s) for s in range(len(env.P))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74bb80-671d-42a6-a58d-33b0a448e875",
   "metadata": {},
   "source": [
    "## Value iteration - by using Q-value one can optimize policy and estimate states values in one call, as opposed to two calls in policy iteration above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "64cec09a-24d0-4a7e-acf4-cac9d2668ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, gamma=1.0, theta=1e-10):\n",
    "    V = np.zeros(len(P), dtype=np.float64)\n",
    "    while True:\n",
    "        Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n",
    "        for s in range(len(P)):\n",
    "            for a in range(len(P[s])):\n",
    "                for prob, next_state, reward, done in P[s][a]:\n",
    "                    # This is the sum over s', the next states fan-out\n",
    "                    # with corresponding proper transition probabilities and rewards\n",
    "                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "        if np.max(np.abs(V - np.max(Q, axis=1))) < theta:\n",
    "            break\n",
    "        V = np.max(Q, axis=1)\n",
    "    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9453b628-ac1b-4ee1-b80f-1cd50306446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_i, pi_i = value_iteration(env.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "818c3bf3-2916-455c-8980-1cc1d0a975ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64, 0.8 , 0.64])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "21fb3ff4-b068-4bb4-bfa4-1adc6ef5889c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.value_iteration.<locals>.<lambda>(s)>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a8ead409-8105-4d2b-963d-fa6ea9a8f492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 1, 2: 0}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{s:pi_i(s) for s in range(len(env.P))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f952c0-a3de-4916-9960-c8207c9d9e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf39",
   "language": "python",
   "name": "tf39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
